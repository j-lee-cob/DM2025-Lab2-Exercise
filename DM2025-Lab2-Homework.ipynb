{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Student Information**\n",
    "Name: Young Jin Lee(李榮振)\n",
    "\n",
    "Student ID: 109006244\n",
    "\n",
    "GitHub ID: j-lee-cob\n",
    "\n",
    "Kaggle name: Jacob0644\n",
    "\n",
    "Kaggle private scoreboard snapshot: \n",
    "\n",
    "![pic_ranking.png](./pics/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Instructions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab we have divided the assignments into **three phases/parts**. The `first two phases` refer to the `exercises inside the Master notebooks` of the [DM2025-Lab2-Exercise Repo](https://github.com/difersalest/DM2025-Lab2-Exercise.git). The `third phase` refers to an `internal Kaggle competition` that we are gonna run among all the Data Mining students. Together they add up to `100 points` of your grade. There are also some `bonus points` to be gained if you complete `extra exercises` in the lab **(bonus 15 pts)** and in the `Kaggle Competition report` **(bonus 5 pts)**.\n",
    "\n",
    "**Environment recommendations to solve lab 2:**\n",
    "- **Phase 1 exercises:** Need GPU for training the models explained in that part, if you don't have a GPU in your laptop it is recommended to run in Colab or Kaggle for a faster experience, although with CPU they can still be solved but with a slower execution.\n",
    "- **Phase 2 exercises:** We use Gemini's API so everything can be run with only CPU without a problem.\n",
    "- **Phase 3 exercises:** For the competition you will probably need GPU to train your models, so it is recommended to use Colab or Kaggle if you don't have a laptop with a dedicated GPU.\n",
    "- **Optional Ollama Notebook (not graded):** You need GPU, at least 4GB of VRAM with 16 GB of RAM to run the local open-source LLM models. \n",
    "\n",
    "## **Phase 1 (30 pts):**\n",
    "\n",
    "1. __Main Exercises (25 pts):__ Do the **take home exercises** from Sections: `1. Data Preparation` to `9. High-dimension Visualization: t-SNE and UMAP`, in the [DM2025-Lab2-Master-Phase_1 Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_1.ipynb). Total: `8 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 3th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "## **Phase 2 (30 pts):**\n",
    "\n",
    "1. **Main Exercises (25 pts):** Do the remaining **take home exercises** from Section: `2. Large Language Models (LLMs)` in the [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb). Total: `5 exercises required from sections 2.1, 2.2, 2.4 and 2.6`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "3. **`Bonus (15 pts):`** Complete the bonus exercises in the [DM2025-Lab2-Master-Phase_2_Bonus Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Bonus.ipynb) and [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb) `where 2 exercises are counted as bonus from sections 2.3 and 2.5 in the main notebook`. Total: `7 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "## **Phase 3 (40 pts):**\n",
    "\n",
    "1. **Kaggle Competition Participation (30 pts):** Participate in the in-class **Kaggle Competition** regarding Emotion Recognition on Twitter by clicking in this link: **[Data Mining Class Kaggle Competition](https://www.kaggle.com/t/3a2df4c6d6b4417e8bf718ed648d7554)**. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20 pts of the 30 pts in this competition participation part.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**. Make sure to take a screenshot of your position at the end of the competition and store it as `pic_ranking.png` under the `pics` folder of this repository and rerun the cell **Student Information**.\n",
    "\n",
    "2. **Competition Report (10 pts)** A report section to be filled in inside this notebook in Markdown Format, we already provided you with the template below. You need to describe your work developing the model for the competition. The report should include a section describing briefly the following elements: \n",
    "* Your preprocessing steps.\n",
    "* The feature engineering steps.\n",
    "* Explanation of your model.\n",
    "\n",
    "* **`Bonus (5 pts):`**\n",
    "    * You will have to describe more detail in the previous steps.\n",
    "    * Mention different things you tried.\n",
    "    * Mention insights you gained. \n",
    "\n",
    "[Markdown Guide - Basic Syntax](https://www.markdownguide.org/basic-syntax/)\n",
    "\n",
    "**`Things to note for Phase 3:`**\n",
    "\n",
    "* **The code used for the competition should be in this Jupyter Notebook File** `DM2025-Lab2-Homework.ipynb`.\n",
    "\n",
    "* **Push the code used for the competition to your repository**.\n",
    "\n",
    "* **The code should have a clear separation for the same sections of the report, preprocessing, feature engineering and model explanation. Briefly comment your code for easier understanding, we provide a template at the end of this notebook.**\n",
    "\n",
    "* Showing the kaggle screenshot of the ranking plus the code in this notebook will ensure the validity of your participation and the report to obtain the corresponding points.\n",
    "\n",
    "After the competition ends you will have two days more to submit the `DM2025-Lab2-Homework.ipynb` with your report in markdown format and your code. Do everything **`BEFORE the deadline (Nov. 26th, 11:59 pm, Wednesday) to obtain 100% of the available points.`**\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding NTU Cool assignment.\n",
    "\n",
    "## **Deadlines:**\n",
    "\n",
    "![lab2_deadlines](./pics/lab2_deadlines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next you will find the template report with some simple markdown syntax explanations, use it to structure your content.\n",
    "\n",
    "You can delete the syntax suggestions after you use them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# **Project Report**\n",
    "\n",
    "**Syntax:** `#` creates the largest heading (H1).\n",
    "\n",
    "---\n",
    "**Syntax:** `---` creates a horizontal rule (a separator line).\n",
    "\n",
    "## 1. Model Development (10 pts Required)\n",
    "\n",
    "**Syntax:** `##` creates a secondary heading (H2).\n",
    "\n",
    "**Describe briefly each section, you can add graphs/charts to support your explanations.**\n",
    "\n",
    "### 1.1 Preprocessing Steps\n",
    "\n",
    "**Syntax:** `###` creates a tertiary heading (H3).\n",
    "\n",
    "[Content for Preprocessing]\n",
    "\n",
    "**Example Syntax for Content:**\n",
    "*   **Bold text:** `**text**`\n",
    "*   *Italic text*: `*text*`\n",
    "*   Bullet point list:\n",
    "    * Item 1\n",
    "    * Item 2\n",
    "\n",
    "Markdown Syntax to Add Image: `![Description of the Image](./your_local_folder/name_of_the_image.png)`\n",
    "\n",
    "![Example Markdown Syntax to Add Image](./pics/example_md_img.png)\n",
    "\n",
    "### 1.2 Feature Engineering Steps\n",
    "\n",
    "[Content for Feature Engineering]\n",
    "\n",
    "### 1.3 Explanation of Your Model\n",
    "\n",
    "[Content for Model Explanation]\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Bonus Section (5 pts Optional)\n",
    "\n",
    "**Add more detail in previous sections**\n",
    "\n",
    "### 2.1 Mention Different Things You Tried\n",
    "\n",
    "[Content for Experiments]\n",
    "\n",
    "### 2.2 Mention Insights You Gained\n",
    "\n",
    "[Content for Insights]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`From here on starts the code section for the competition.`**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": [
    "##Project Report\n",
    "\n",
    "###Preprocessing Step 1.1\n",
    "**Steps performed:**\n",
    "\n",
    "- Lowercasing text- All text was converted to lowercase to reduce duplicate vocabulary.\n",
    "\n",
    "- Tokenization- Since the baseline model uses TF-IDF, I used simple whitespace tokenization, sufficient for bag-of-words style features.\n",
    "\n",
    "- Removing emptyrows- Rows with missing text or invalid entries were removed.\n",
    "\n",
    "- Validation split- The dataset was split into training and validation sets using a stratified split based on emotion labels, so each emotion appears in both sets with roughly similar proportions.\n",
    "\n",
    "###Feature Engineering step 1.2\n",
    "\n",
    "-Because models like Logistic Regression cannot directly read raw text, this process convert text into numeric form.\n",
    "\n",
    "-Main feature representation\n",
    "\n",
    "-TF-IDF Vectorization- TF-IDF transforms text into a sparse numeric vector where words that are frequent in a specific text but not overly common across the whole dataset become more important. This is a strong baseline for many text classification tasks.\n",
    "\n",
    "Additional handcrafted features:\n",
    "\n",
    "    -char_count: number of characters in the text\n",
    "\n",
    "    -word_count: number of words in the text\n",
    "\n",
    "    -num_hashtags: number of hashtags in the text\n",
    "\n",
    "These extra features were added because short social posts often express emotions through writing style (very short angry text vs long explanatory sad text, heavy hashtag usage). After extracting TF-IDF features, I concatenated the TF-IDF vectors with these numeric features to form the final model input.\n",
    "\n",
    "\n",
    "###Explanation of your model 1.3\n",
    "**For the baseline classifier, I used Logistic Regression for multi-class emotion prediction.**\n",
    "\n",
    "Why Logistic Regression?\n",
    "\n",
    "    It has strong baseline for sparse text features. TF-IDF produces very high-dimensional sparse vectors, and Logistic Regression typically performs well in that setting.\n",
    "\n",
    "    Multi-class handling-  Multi-class classification is handled using a softmax-style approach with solver='lbfgs'.\n",
    "\n",
    "    Class imbalance handling- class_weight='balanced' helps prevent the model from over-focusing on majority emotions by automatically giving more weight to underrepresented classes.\n",
    "\n",
    "    Evaluation metrics- I reported Accuracy plus Precision, Recall, and F1-score per emotion, since accuracy alone can look “good” even when minority classes are being ignored.\n",
    "\n",
    "\n",
    "Observations from results\n",
    "\n",
    "More common emotions (for example joy) generally show higher recall, since the model sees many training examples and can learn clearer patterns.\n",
    "\n",
    "Minority emotions (for example disgust or fear) tend to perform worse because there are fewer examples and the language are varied.\n",
    "\n",
    "Adding char_count, word_count, and num_hashtags can provide small gains in certain settings, but improvements are often modest compared to the impact of better text representations.\n",
    "\n",
    "\n",
    "##Bonus Section\n",
    "\n",
    "##2.1 Mention the things you tried\n",
    "\n",
    "- To make sure the final approach wasn’t chosen blindly, I tested multiple variations across model choice, preprocessing, and feature size:\n",
    "\n",
    "    1) Different classifiers\n",
    "\n",
    "    Logistic Regression (with W)- main baseline because it fits TF-IDF well.\n",
    "\n",
    "    Decision Tree- mainly for interpretability , but it can overfit and usually struggles with high-dimensional sparse features.\n",
    "\n",
    "    Random Forest / Gradient Boosting (optional)- tested as stronger non-linear baselines. These can sometimes improve performance but are heavier and not always ideal with sparse TF-IDF unless tuned carefully.\n",
    "\n",
    "    2) TF-IDF configuration tuning\n",
    "\n",
    "    Tried different vocabulary sizes (e.g., 10k / 20k / 50k features) to see the trade-off between:\n",
    "\n",
    "    too small → missing useful words/hashtags,\n",
    "\n",
    "    too large → more noise + slower training.\n",
    "\n",
    "    3) Extra features beyond text\n",
    "\n",
    "    char_count, word_count, num_hashtags\n",
    "\n",
    "    Motivation- emotion can correlate with intensity markers (length, emphasis, and hashtag patterns).\n",
    "\n",
    "    4) Preprocessing variations\n",
    "\n",
    "    Keeping emojis vs removing emojis: emojis can carry emotion directly, so keeping them may help depending on how they appear in the dataset.\n",
    "\n",
    "    Including hashtags inside the main text- hashtags often contain emotion keywords, merging them into text can boost TF-IDF usefulness.\n",
    "\n",
    "    Stopword removal vs keeping stopwords: sometimes stopwords are not helpful, but in short emotional text they can still carry patterns so it’s worth testing instead of assuming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Competition Code**\n",
    "\n",
    "## 1. Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>root._type</th>\n",
       "      <th>root._source.post.post_id</th>\n",
       "      <th>root._source.post.text</th>\n",
       "      <th>root._source.post.hashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>post</td>\n",
       "      <td>0x61fc95</td>\n",
       "      <td>We got the ranch, loaded our guns and sat up t...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>post</td>\n",
       "      <td>0x35663e</td>\n",
       "      <td>I bet there is an army of married couples who ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>post</td>\n",
       "      <td>0xc78afe</td>\n",
       "      <td>This could only end badly.</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>post</td>\n",
       "      <td>0x90089c</td>\n",
       "      <td>My sister squeezed a lime in her milk when she...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>post</td>\n",
       "      <td>0xaba820</td>\n",
       "      <td>and that got my head bobbing a little bit.</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64166</th>\n",
       "      <td>post</td>\n",
       "      <td>0x4afbe1</td>\n",
       "      <td>Guilty Gear actually did that before with Guil...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64167</th>\n",
       "      <td>post</td>\n",
       "      <td>0xf5ba78</td>\n",
       "      <td>One of my favorite episodes.</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64168</th>\n",
       "      <td>post</td>\n",
       "      <td>0x8f758e</td>\n",
       "      <td>I got my first raspberry from a crowd surfer f...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64169</th>\n",
       "      <td>post</td>\n",
       "      <td>0xb5a35a</td>\n",
       "      <td>Texans and Astros both shut out tonight. Houst...</td>\n",
       "      <td>[texans, astros, sadness, losers]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64170</th>\n",
       "      <td>post</td>\n",
       "      <td>0x3a9174</td>\n",
       "      <td>Pre-prepare direction plays hale and hearty si...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64171 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      root._type root._source.post.post_id  \\\n",
       "0           post                  0x61fc95   \n",
       "1           post                  0x35663e   \n",
       "2           post                  0xc78afe   \n",
       "3           post                  0x90089c   \n",
       "4           post                  0xaba820   \n",
       "...          ...                       ...   \n",
       "64166       post                  0x4afbe1   \n",
       "64167       post                  0xf5ba78   \n",
       "64168       post                  0x8f758e   \n",
       "64169       post                  0xb5a35a   \n",
       "64170       post                  0x3a9174   \n",
       "\n",
       "                                  root._source.post.text  \\\n",
       "0      We got the ranch, loaded our guns and sat up t...   \n",
       "1      I bet there is an army of married couples who ...   \n",
       "2                             This could only end badly.   \n",
       "3      My sister squeezed a lime in her milk when she...   \n",
       "4             and that got my head bobbing a little bit.   \n",
       "...                                                  ...   \n",
       "64166  Guilty Gear actually did that before with Guil...   \n",
       "64167                       One of my favorite episodes.   \n",
       "64168  I got my first raspberry from a crowd surfer f...   \n",
       "64169  Texans and Astros both shut out tonight. Houst...   \n",
       "64170  Pre-prepare direction plays hale and hearty si...   \n",
       "\n",
       "              root._source.post.hashtags  \n",
       "0                                     []  \n",
       "1                                     []  \n",
       "2                                     []  \n",
       "3                                     []  \n",
       "4                                     []  \n",
       "...                                  ...  \n",
       "64166                                 []  \n",
       "64167                                 []  \n",
       "64168                                 []  \n",
       "64169  [texans, astros, sadness, losers]  \n",
       "64170                                 []  \n",
       "\n",
       "[64171 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Add the code related to the preprocessing steps in cells inside this section\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "with open(\"final_posts.json\", \"r\",encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Flatten the nested JSON structure\n",
    "df = pd.json_normalize(data, record_path=None)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x61fc95</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x35663e</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0xc78afe</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x90089c</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0xaba820</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64166</th>\n",
       "      <td>0x4afbe1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64167</th>\n",
       "      <td>0xf5ba78</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64168</th>\n",
       "      <td>0x8f758e</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64169</th>\n",
       "      <td>0xb5a35a</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64170</th>\n",
       "      <td>0x3a9174</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64171 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  split\n",
       "0      0x61fc95   test\n",
       "1      0x35663e  train\n",
       "2      0xc78afe  train\n",
       "3      0x90089c  train\n",
       "4      0xaba820   test\n",
       "...         ...    ...\n",
       "64166  0x4afbe1  train\n",
       "64167  0xf5ba78  train\n",
       "64168  0x8f758e   test\n",
       "64169  0xb5a35a  train\n",
       "64170  0x3a9174   test\n",
       "\n",
       "[64171 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data_identification\n",
    "df_id = pd.read_csv(\"data_identification.csv\")\n",
    "df_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x35663e</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0xc78afe</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x90089c</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x2ffb63</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x989146</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47885</th>\n",
       "      <td>0xd740f2</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47886</th>\n",
       "      <td>0x99267e</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47887</th>\n",
       "      <td>0x4afbe1</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47888</th>\n",
       "      <td>0xf5ba78</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47889</th>\n",
       "      <td>0xb5a35a</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47890 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  emotion\n",
       "0      0x35663e      joy\n",
       "1      0xc78afe     fear\n",
       "2      0x90089c      joy\n",
       "3      0x2ffb63      joy\n",
       "4      0x989146      joy\n",
       "...         ...      ...\n",
       "47885  0xd740f2      joy\n",
       "47886  0x99267e    anger\n",
       "47887  0x4afbe1    anger\n",
       "47888  0xf5ba78      joy\n",
       "47889  0xb5a35a  sadness\n",
       "\n",
       "[47890 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_em = pd.read_csv(\"emotion.csv\")\n",
    "df_em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>split</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x61fc95</td>\n",
       "      <td>We got the ranch, loaded our guns and sat up t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x35663e</td>\n",
       "      <td>I bet there is an army of married couples who ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0xc78afe</td>\n",
       "      <td>This could only end badly.</td>\n",
       "      <td>[]</td>\n",
       "      <td>train</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x90089c</td>\n",
       "      <td>My sister squeezed a lime in her milk when she...</td>\n",
       "      <td>[]</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0xaba820</td>\n",
       "      <td>and that got my head bobbing a little bit.</td>\n",
       "      <td>[]</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64166</th>\n",
       "      <td>0x4afbe1</td>\n",
       "      <td>Guilty Gear actually did that before with Guil...</td>\n",
       "      <td>[]</td>\n",
       "      <td>train</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64167</th>\n",
       "      <td>0xf5ba78</td>\n",
       "      <td>One of my favorite episodes.</td>\n",
       "      <td>[]</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64168</th>\n",
       "      <td>0x8f758e</td>\n",
       "      <td>I got my first raspberry from a crowd surfer f...</td>\n",
       "      <td>[]</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64169</th>\n",
       "      <td>0xb5a35a</td>\n",
       "      <td>Texans and Astros both shut out tonight. Houst...</td>\n",
       "      <td>[texans, astros, sadness, losers]</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64170</th>\n",
       "      <td>0x3a9174</td>\n",
       "      <td>Pre-prepare direction plays hale and hearty si...</td>\n",
       "      <td>[]</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64171 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                               text  \\\n",
       "0      0x61fc95  We got the ranch, loaded our guns and sat up t...   \n",
       "1      0x35663e  I bet there is an army of married couples who ...   \n",
       "2      0xc78afe                         This could only end badly.   \n",
       "3      0x90089c  My sister squeezed a lime in her milk when she...   \n",
       "4      0xaba820         and that got my head bobbing a little bit.   \n",
       "...         ...                                                ...   \n",
       "64166  0x4afbe1  Guilty Gear actually did that before with Guil...   \n",
       "64167  0xf5ba78                       One of my favorite episodes.   \n",
       "64168  0x8f758e  I got my first raspberry from a crowd surfer f...   \n",
       "64169  0xb5a35a  Texans and Astros both shut out tonight. Houst...   \n",
       "64170  0x3a9174  Pre-prepare direction plays hale and hearty si...   \n",
       "\n",
       "                                hashtags  split  emotion  \n",
       "0                                     []   test      NaN  \n",
       "1                                     []  train      joy  \n",
       "2                                     []  train     fear  \n",
       "3                                     []  train      joy  \n",
       "4                                     []   test      NaN  \n",
       "...                                  ...    ...      ...  \n",
       "64166                                 []  train    anger  \n",
       "64167                                 []  train      joy  \n",
       "64168                                 []   test      NaN  \n",
       "64169  [texans, astros, sadness, losers]  train  sadness  \n",
       "64170                                 []   test      NaN  \n",
       "\n",
       "[64171 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_posts = df.rename(columns={\n",
    "    \"root._source.post.post_id\": \"id\",\n",
    "\n",
    "    \"root._source.post.text\": \"text\",\n",
    "\n",
    "    \"root._source.post.hashtags\": \"hashtags\"\n",
    "\n",
    "})[[\"id\", \"text\", \"hashtags\"]]  \n",
    "\n",
    "#attach df_id info using id\n",
    "df_merged = df_posts.merge(df_id, on=\"id\", how=\"left\")\n",
    "\n",
    "#emotion/label info using id\n",
    "df_merged = df_merged.merge(df_em, on=\"id\", how=\"left\")\n",
    "\n",
    "df_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>split</th>\n",
       "      <th>emotion</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x61fc95</td>\n",
       "      <td>We got the ranch, loaded our guns and sat up t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "      <td>we got the ranch, loaded our guns and sat up t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x35663e</td>\n",
       "      <td>I bet there is an army of married couples who ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "      <td>i bet there is an army of married couples who ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0xc78afe</td>\n",
       "      <td>This could only end badly.</td>\n",
       "      <td>[]</td>\n",
       "      <td>train</td>\n",
       "      <td>fear</td>\n",
       "      <td>this could only end badly.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x90089c</td>\n",
       "      <td>My sister squeezed a lime in her milk when she...</td>\n",
       "      <td>[]</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "      <td>my sister squeezed a lime in her milk when she...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0xaba820</td>\n",
       "      <td>and that got my head bobbing a little bit.</td>\n",
       "      <td>[]</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "      <td>and that got my head bobbing a little bit.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64166</th>\n",
       "      <td>0x4afbe1</td>\n",
       "      <td>Guilty Gear actually did that before with Guil...</td>\n",
       "      <td>[]</td>\n",
       "      <td>train</td>\n",
       "      <td>anger</td>\n",
       "      <td>guilty gear actually did that before with guil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64167</th>\n",
       "      <td>0xf5ba78</td>\n",
       "      <td>One of my favorite episodes.</td>\n",
       "      <td>[]</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "      <td>one of my favorite episodes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64168</th>\n",
       "      <td>0x8f758e</td>\n",
       "      <td>I got my first raspberry from a crowd surfer f...</td>\n",
       "      <td>[]</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i got my first raspberry from a crowd surfer f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64169</th>\n",
       "      <td>0xb5a35a</td>\n",
       "      <td>Texans and Astros both shut out tonight. Houst...</td>\n",
       "      <td>[texans, astros, sadness, losers]</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "      <td>texans and astros both shut out tonight. houst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64170</th>\n",
       "      <td>0x3a9174</td>\n",
       "      <td>Pre-prepare direction plays hale and hearty si...</td>\n",
       "      <td>[]</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pre-prepare direction plays hale and hearty si...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64171 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                               text  \\\n",
       "0      0x61fc95  We got the ranch, loaded our guns and sat up t...   \n",
       "1      0x35663e  I bet there is an army of married couples who ...   \n",
       "2      0xc78afe                         This could only end badly.   \n",
       "3      0x90089c  My sister squeezed a lime in her milk when she...   \n",
       "4      0xaba820         and that got my head bobbing a little bit.   \n",
       "...         ...                                                ...   \n",
       "64166  0x4afbe1  Guilty Gear actually did that before with Guil...   \n",
       "64167  0xf5ba78                       One of my favorite episodes.   \n",
       "64168  0x8f758e  I got my first raspberry from a crowd surfer f...   \n",
       "64169  0xb5a35a  Texans and Astros both shut out tonight. Houst...   \n",
       "64170  0x3a9174  Pre-prepare direction plays hale and hearty si...   \n",
       "\n",
       "                                hashtags  split  emotion  \\\n",
       "0                                     []   test      NaN   \n",
       "1                                     []  train      joy   \n",
       "2                                     []  train     fear   \n",
       "3                                     []  train      joy   \n",
       "4                                     []   test      NaN   \n",
       "...                                  ...    ...      ...   \n",
       "64166                                 []  train    anger   \n",
       "64167                                 []  train      joy   \n",
       "64168                                 []   test      NaN   \n",
       "64169  [texans, astros, sadness, losers]  train  sadness   \n",
       "64170                                 []   test      NaN   \n",
       "\n",
       "                                              clean_text  \n",
       "0      we got the ranch, loaded our guns and sat up t...  \n",
       "1      i bet there is an army of married couples who ...  \n",
       "2                             this could only end badly.  \n",
       "3      my sister squeezed a lime in her milk when she...  \n",
       "4             and that got my head bobbing a little bit.  \n",
       "...                                                  ...  \n",
       "64166  guilty gear actually did that before with guil...  \n",
       "64167                       one of my favorite episodes.  \n",
       "64168  i got my first raspberry from a crowd surfer f...  \n",
       "64169  texans and astros both shut out tonight. houst...  \n",
       "64170  pre-prepare direction plays hale and hearty si...  \n",
       "\n",
       "[64171 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning data: lowercase + remove URLs + remove mentions + normalize spaces\n",
    "import re\n",
    "\n",
    "\n",
    "URL_RE = re.compile(r\"(http\\S+|www\\S+)\")\n",
    "\n",
    "MENTION_RE = re.compile(r\"@\\w+\")\n",
    "\n",
    "SPACE_RE = re.compile(r\"\\s+\")\n",
    "\n",
    "def clean_text(t: str) -> str:\n",
    "    t = str(t).lower()\n",
    "\n",
    "    t = URL_RE.sub(\"\", t)          \n",
    "\n",
    "    t = MENTION_RE.sub(\"\", t)      \n",
    "\n",
    "    t = SPACE_RE.sub(\" \", t).strip()\n",
    "\n",
    "\n",
    "    return t\n",
    "\n",
    "\n",
    "df_merged[\"clean_text\"] = df_merged[\"text\"].apply(clean_text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_merged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>split</th>\n",
       "      <th>emotion</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>hashtags_processed</th>\n",
       "      <th>hashtags_text</th>\n",
       "      <th>text_plus_hashtag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x61fc95</td>\n",
       "      <td>We got the ranch, loaded our guns and sat up t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "      <td>we got the ranch, loaded our guns and sat up t...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>we got the ranch, loaded our guns and sat up t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0x35663e</td>\n",
       "      <td>I bet there is an army of married couples who ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "      <td>i bet there is an army of married couples who ...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>i bet there is an army of married couples who ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0xc78afe</td>\n",
       "      <td>This could only end badly.</td>\n",
       "      <td>[]</td>\n",
       "      <td>train</td>\n",
       "      <td>fear</td>\n",
       "      <td>this could only end badly.</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>this could only end badly.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0x90089c</td>\n",
       "      <td>My sister squeezed a lime in her milk when she...</td>\n",
       "      <td>[]</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "      <td>my sister squeezed a lime in her milk when she...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>my sister squeezed a lime in her milk when she...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0xaba820</td>\n",
       "      <td>and that got my head bobbing a little bit.</td>\n",
       "      <td>[]</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "      <td>and that got my head bobbing a little bit.</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>and that got my head bobbing a little bit.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64166</th>\n",
       "      <td>0x4afbe1</td>\n",
       "      <td>Guilty Gear actually did that before with Guil...</td>\n",
       "      <td>[]</td>\n",
       "      <td>train</td>\n",
       "      <td>anger</td>\n",
       "      <td>guilty gear actually did that before with guil...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>guilty gear actually did that before with guil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64167</th>\n",
       "      <td>0xf5ba78</td>\n",
       "      <td>One of my favorite episodes.</td>\n",
       "      <td>[]</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "      <td>one of my favorite episodes.</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>one of my favorite episodes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64168</th>\n",
       "      <td>0x8f758e</td>\n",
       "      <td>I got my first raspberry from a crowd surfer f...</td>\n",
       "      <td>[]</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i got my first raspberry from a crowd surfer f...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>i got my first raspberry from a crowd surfer f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64169</th>\n",
       "      <td>0xb5a35a</td>\n",
       "      <td>Texans and Astros both shut out tonight. Houst...</td>\n",
       "      <td>[texans, astros, sadness, losers]</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "      <td>texans and astros both shut out tonight. houst...</td>\n",
       "      <td>[texans, astros, sadness, losers]</td>\n",
       "      <td>texans astros sadness losers</td>\n",
       "      <td>texans and astros both shut out tonight. houst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64170</th>\n",
       "      <td>0x3a9174</td>\n",
       "      <td>Pre-prepare direction plays hale and hearty si...</td>\n",
       "      <td>[]</td>\n",
       "      <td>test</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pre-prepare direction plays hale and hearty si...</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td>pre-prepare direction plays hale and hearty si...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>64171 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                               text  \\\n",
       "0      0x61fc95  We got the ranch, loaded our guns and sat up t...   \n",
       "1      0x35663e  I bet there is an army of married couples who ...   \n",
       "2      0xc78afe                         This could only end badly.   \n",
       "3      0x90089c  My sister squeezed a lime in her milk when she...   \n",
       "4      0xaba820         and that got my head bobbing a little bit.   \n",
       "...         ...                                                ...   \n",
       "64166  0x4afbe1  Guilty Gear actually did that before with Guil...   \n",
       "64167  0xf5ba78                       One of my favorite episodes.   \n",
       "64168  0x8f758e  I got my first raspberry from a crowd surfer f...   \n",
       "64169  0xb5a35a  Texans and Astros both shut out tonight. Houst...   \n",
       "64170  0x3a9174  Pre-prepare direction plays hale and hearty si...   \n",
       "\n",
       "                                hashtags  split  emotion  \\\n",
       "0                                     []   test      NaN   \n",
       "1                                     []  train      joy   \n",
       "2                                     []  train     fear   \n",
       "3                                     []  train      joy   \n",
       "4                                     []   test      NaN   \n",
       "...                                  ...    ...      ...   \n",
       "64166                                 []  train    anger   \n",
       "64167                                 []  train      joy   \n",
       "64168                                 []   test      NaN   \n",
       "64169  [texans, astros, sadness, losers]  train  sadness   \n",
       "64170                                 []   test      NaN   \n",
       "\n",
       "                                              clean_text  \\\n",
       "0      we got the ranch, loaded our guns and sat up t...   \n",
       "1      i bet there is an army of married couples who ...   \n",
       "2                             this could only end badly.   \n",
       "3      my sister squeezed a lime in her milk when she...   \n",
       "4             and that got my head bobbing a little bit.   \n",
       "...                                                  ...   \n",
       "64166  guilty gear actually did that before with guil...   \n",
       "64167                       one of my favorite episodes.   \n",
       "64168  i got my first raspberry from a crowd surfer f...   \n",
       "64169  texans and astros both shut out tonight. houst...   \n",
       "64170  pre-prepare direction plays hale and hearty si...   \n",
       "\n",
       "                      hashtags_processed                 hashtags_text  \\\n",
       "0                                     []                                 \n",
       "1                                     []                                 \n",
       "2                                     []                                 \n",
       "3                                     []                                 \n",
       "4                                     []                                 \n",
       "...                                  ...                           ...   \n",
       "64166                                 []                                 \n",
       "64167                                 []                                 \n",
       "64168                                 []                                 \n",
       "64169  [texans, astros, sadness, losers]  texans astros sadness losers   \n",
       "64170                                 []                                 \n",
       "\n",
       "                                       text_plus_hashtag  \n",
       "0      we got the ranch, loaded our guns and sat up t...  \n",
       "1      i bet there is an army of married couples who ...  \n",
       "2                             this could only end badly.  \n",
       "3      my sister squeezed a lime in her milk when she...  \n",
       "4             and that got my head bobbing a little bit.  \n",
       "...                                                  ...  \n",
       "64166  guilty gear actually did that before with guil...  \n",
       "64167                       one of my favorite episodes.  \n",
       "64168  i got my first raspberry from a crowd surfer f...  \n",
       "64169  texans and astros both shut out tonight. houst...  \n",
       "64170  pre-prepare direction plays hale and hearty si...  \n",
       "\n",
       "[64171 rows x 9 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Add the code related to the feature engineering steps in cells inside this section\n",
    "#Combine clean text + hashtags into one column\n",
    "\n",
    "#Make sure hashtags are lists (non-list -> empty list)\n",
    "df_merged[\"hashtags_processed\"] = df_merged[\"hashtags\"].apply(\n",
    "\n",
    "lambda x: x if isinstance(x, list) else []\n",
    ")\n",
    "\n",
    "\n",
    "#Turn hashtag lists into a single string ([\"a\",\"b\"] -> \"a b\")\n",
    "df_merged[\"hashtags_text\"] = df_merged[\"hashtags_processed\"].apply(\n",
    "\n",
    "lambda tags: \" \".join(tags) if tags else \"\"\n",
    ")\n",
    "\n",
    "\n",
    "# Combine clean_text + hashtags_text\n",
    "df_merged[\"text_plus_hashtag\"] = (\n",
    "\n",
    "    df_merged[\"clean_text\"].fillna(\"\").astype(str)\n",
    "    + \" \"\n",
    "\n",
    "    + df_merged[\"hashtags_text\"].fillna(\"\").astype(str)\n",
    ").str.strip()\n",
    "\n",
    "\n",
    "\n",
    "df_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping: {'anger': 0, 'disgust': 1, 'fear': 2, 'joy': 3, 'nan': 4, 'sadness': 5, 'surprise': 6}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode emotion labels into integers\n",
    "le = LabelEncoder()\n",
    "\n",
    "df_merged[\"emotion_id\"] = le.fit_transform(df_merged[\"emotion\"].astype(str))\n",
    "\n",
    "# Build a readable mapping: {label_name: label_id}\n",
    "label_mapping = {label: idx for idx, label in enumerate(le.classes_)}\n",
    "\n",
    "print(\"Label mapping:\", label_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id                                  text_plus_hashtag  emotion_id\n",
      "1  0x35663e  i bet there is an army of married couples who ...           3\n",
      "2  0xc78afe                         this could only end badly.           2\n",
      "3  0x90089c  my sister squeezed a lime in her milk when she...           3\n",
      "7  0x2ffb63                                thank you so much❤️           3\n",
      "9  0x989146  stinks because ive been in this program for a ...           3\n",
      "         id                                  text_plus_hashtag\n",
      "0  0x61fc95  we got the ranch, loaded our guns and sat up t...\n",
      "4  0xaba820         and that got my head bobbing a little bit.\n",
      "5  0x66e44d                same. glad it's not just out store.\n",
      "6  0xc03cf5  like always i will wait and see thanks for the...\n",
      "8  0x02f65a  there's a bit of room between \"not loving sub-...\n"
     ]
    }
   ],
   "source": [
    "#Split into train/test using the existing \"split\" column\n",
    "train_df = df_merged.loc[df_merged[\"split\"].eq(\"train\")].copy()\n",
    "test_df  = df_merged.loc[df_merged[\"split\"].eq(\"test\")].copy()\n",
    "\n",
    "#Drop columns we want clean style\n",
    "train_drop_cols = [ \"text\", \"split\", \"hashtags\", \"hashtags_processed\", \"clean_text\", \"hashtags_text\", \"emotion\"]\n",
    "test_drop_cols = [\n",
    "    \"text\", \"split\", \"emotion\", \"hashtags\", \"hashtags_processed\", \"clean_text\", \"hashtags_text\",  \"emotion_id\"]\n",
    "\n",
    "train_df = train_df.drop(columns=train_drop_cols, errors=\"ignore\")\n",
    "\n",
    "test_df  = test_df.drop(columns=test_drop_cols,  errors=\"ignore\")\n",
    "\n",
    "print(train_df.head())\n",
    "\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/youngjinlee/Desktop/DM2025-Lab2-Exercise/.venv/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "\n",
    "#Split labeled data into train/validation \n",
    "train_df, val_df = train_test_split(\n",
    "    train_df,\n",
    "\n",
    "    test_size=0.10,\n",
    "\n",
    "    random_state=42,\n",
    "    stratify=train_df[\"emotion_id\"]\n",
    ")\n",
    "\n",
    "#TF-IDF turns text into sparse numeric features \n",
    "tfidf = TfidfVectorizer(\n",
    "\n",
    "    max_features=100_000,\n",
    "\n",
    "    tokenizer=nltk.word_tokenize\n",
    ")\n",
    "\n",
    "X_train = tfidf.fit_transform(train_df[\"text_plus_hashtag\"])  #learn vocab and  weights from train only\n",
    "\n",
    "X_val   = tfidf.transform(val_df[\"text_plus_hashtag\"])        #reuse same vocab, weights on val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Implementation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.51      0.53      1069\n",
      "           1       0.15      0.32      0.21       118\n",
      "           2       0.31      0.55      0.40       201\n",
      "           3       0.83      0.63      0.72      2380\n",
      "           5       0.34      0.47      0.39       393\n",
      "           6       0.44      0.58      0.50       628\n",
      "\n",
      "    accuracy                           0.57      4789\n",
      "   macro avg       0.44      0.51      0.46      4789\n",
      "weighted avg       0.64      0.57      0.59      4789\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Add the code related to the model implementation steps in cells inside this section\n",
    "\n",
    "#Logistic Regression (multi-class) + evaluation \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "#Labels we want to predict\n",
    "y_train = train_df[\"emotion_id\"]\n",
    "\n",
    "y_val   = val_df[\"emotion_id\"]\n",
    "\n",
    "#Model setup\n",
    "model = LogisticRegression(\n",
    "    max_iter=10_000,\n",
    "\n",
    "    class_weight=\"balanced\",\n",
    "    solver=\"lbfgs\"\n",
    ")\n",
    "\n",
    "#Train\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#Validate + report\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "print(classification_report(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as test_predictions.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x61fc95</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0xaba820</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0x66e44d</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0xc03cf5</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0x02f65a</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id emotion\n",
       "0  0x61fc95    fear\n",
       "4  0xaba820    fear\n",
       "5  0x66e44d     joy\n",
       "6  0xc03cf5     joy\n",
       "8  0x02f65a   anger"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "clf = model\n",
    "\n",
    "X_test = tfidf.transform(test_df[\"text_plus_hashtag\"])\n",
    "\n",
    "\n",
    "\n",
    "test_pred_ids = clf.predict(X_test)\n",
    "\n",
    "# emotion_id -> emotion string (no hard-coded dict needed)\n",
    "test_pred_labels = le.inverse_transform(test_pred_ids)\n",
    "\n",
    "\n",
    "# Attach to dataframe\n",
    "test_df[\"emotion\"] = test_pred_labels\n",
    "\n",
    "\n",
    "\n",
    "output_df = test_df.loc[:, [\"id\", \"emotion\"]]\n",
    "\n",
    "output_df.to_csv(\"test_predictions.csv\", index=False)\n",
    "\n",
    "print(\"Saved as test_predictions.csv\")\n",
    "\n",
    "output_df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dm2025lab2)",
   "language": "python",
   "name": "dm2025lab2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
